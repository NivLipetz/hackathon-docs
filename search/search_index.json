{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Jarvis Docs","text":""},{"location":"#intro","title":"Intro","text":"<p>Welcome to Jarvis documentation. Jarvis is an abstraction layer, built for GPO 2023 Gen AI Hackathon.</p> <p>Jarvis mission is to create an easy-to-use API that expose different foundation models offered by AWS Bedrock, To be consumed easily and efficiently to enable the hackathon teams focus on their solution rather that implementing a common patterns in the Gen AI world.</p> <p>Jarvis supports 3 types of functionalities:</p> <ol> <li>Zero shot prompts that is passed to the foundation model by your choice.</li> <li>Ability to send prompts on private GPO data that is uploaded prior to the prompt (RAG Pattern).</li> <li>Ability to have a chat conversation with the foundation model where jarvis will persist the chat history for you.</li> </ol> <p>In addition to that, Jarvis supports the upload of a private GPO data that will be stored safely on GPO AWS account and will be enabled to send queries around it (RAG + Chat)</p> <p>More about this API's and how to upload private data in this documentation site.</p>"},{"location":"RAG/","title":"RAG","text":""},{"location":"RAG/#rag-pattern","title":"RAG Pattern","text":"<p>While zero-shot is a nice and give you a good head start, the biggest benefit of using Jarvis on top of Bedrock is Jarvis ability to use embeddings and the RAG pattern.</p> <p>So far what we went on the zero shot part is asking a question, while you can also provide some document text in the prompt.</p> <p></p> <p>As you can see this is very limited functionality because you will have to find the relevant document by your self and the document size is also limited to the max tokens the FM supports.</p>"},{"location":"RAG/#rag-for-the-resource","title":"RAG for the resource","text":"<p>RAG -  Retrieval Augmented Generation RAG improves upon the first where we concatenate our questions with as much relevant context as possible, which is likely to contain the answers or information we are looking for. The challenge here, There is a limit on how much contextual information can be used is determined by the token limit of the model. This can be overcome by using Retrieval Augmented Generation (RAG)</p> <p></p> <p>Let's go over what we see here and start with phase 2</p>"},{"location":"RAG/#uploading-files-2","title":"Uploading files (2)","text":"<p>Jarvis supports uploading files and taking care of everything that happens. Jarvis provides the abilities to upload files from SFTP, confluence and webpages. When you upload files through Jarvis API, Jarvis will do the following: 1. Get the file (SFTP/Confluence/Webpages) 2. Split the docs into chunks 3. Invoke Amazon titan embeddings model to create a vector from the doc chunk 4. Store the vector created by the Amazon titan embeddings model inside Postgres with pgvector support</p> <p>SPANISH SUPPORTED: Amazon titan embeddings model support working on the spanish language as well, which means you can upload docs in spanish and ask question about them in spanish</p>"},{"location":"RAG/#prompt-with-rag-13","title":"Prompt with RAG (1+3)","text":"<p>When jarvis is being asked a question using the prompt API, Jarvis will create a vector from the question  and will find for us the most similar vectors in the Postgres DB. only then Jarvis will invoke bedrock model together with the docs it found as the context.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>Jarvis API</p>"},{"location":"bedrock/","title":"Bedrock","text":""},{"location":"bedrock/#bedrock","title":"Bedrock","text":"<p>Bedrock is a new service that as of writing this line (25.9.23) is still in beta stage and not available to AWS customers.</p> <p>Bedrock is a product by AWS that Accelerates development of generative AI applications using FMs through an API, without managing infrastructure. Currently, Bedrock provides access to foundation model from Anthropic, AI21, AWS Titan family and Stability AI. Bedrock also enables us to privately customize FMs using our organization's data.</p> <p>In the hackathon, we choose to expose those foundation model through Jarvis.</p> Model Name Max Tokens Model Id Notes Classification Claude v1.3 12k anthropic.claude-v1 Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following. Text generation, Conversational Claude v2 12k anthropic.claude-v2 Anthropic's most powerful model, which excels at a wide range of tasks from sophisticated dialogue and creative content generation to detailed instruction following. Text generation, Conversational Claude Instant   v1.1 9k anthropic.claude-instant-v1 A faster and cheaper yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document question-answering. Text generation, Conversational Jurassic-2 Mid 8k ai21.j2-mid Jurassic-2 Mid is AI21\u2019s mid-sized model, carefully designed to strike the right balance between exceptional quality and affordability. Jurassic-2 Mid can be applied to any language comprehension or generation task including question answering, summarization, long-form copy generation, advanced information extraction and many others. Text, Classification, Insert/edit, Math Jurassic-2 Ultra 8k Jurassic-2 Ultra Jurassic-2 Ultra is AI21\u2019s most powerful model offering exceptional quality. Apply Jurassic-2 Ultra to complex tasks that require advanced text generation and comprehension. Popular use cases include question answering, summarization, long-form copy generation, advanced information extraction, and more. Text, Classification, Insert/edit Titan Text Large 8k amazon.titan-tg1-large Titan Text is a generative large language model (LLM) for tasks such as summarization, text generation (for example, creating a blog post), classification, open-ended Q&amp;A, and information extraction. Text generation, Code generation, Instruction following <p>As you can see, each model has different size of tokens and different use, you may need to play with your  hackathon product to find the right model for the job. When using the API, you will be able to change the FM that in use by playing with the model_id.</p> <p>NOTE: 1 token is equal to 3/4 of a word or 4 chars .</p>"},{"location":"bedrock/#why-bedrock-and-not-chatgpt","title":"Why Bedrock and not chatGPT?","text":"<p>One of the biggest advantages of bedrock over other foundation models as chatGPT or Bard is that Bedrock allows us to access to multiple foundation model as noted above but not only that, it allows us to use our private data as context to the foundation model. more about this on the RAG part.</p>"},{"location":"load/","title":"Loading Documents","text":"<p>Jarvis supports loading documents from 3 data sources:       1. SFTP 2. Confluence 3. Webpages </p>"},{"location":"load/#sftp","title":"SFTP","text":"<p>Your team will not only receive a private-key and app-id for the API, but it will also receive a credentials to use SFTP server.  Private key  Username  Password  Address  </p> <p>Once you connect to the sftp, you will have your own folder where you can upload your files. A recommended SFTP client for mac can be Cyberduck</p> <p></p> <p>in this example we uploaded multiple files to the sftp folder</p> <p>NOTE: At this point in time, those file are not yet known to Jarvis, and not stored in Postgres as vectors. They are only stored in the SFTP.</p>"},{"location":"load/#loading-the-files-from-sftp-to-jarvis","title":"Loading the files from SFTP to jarvis","text":"<p>Jarvis is connected to the sftp and is able via API to get a trigger and upload specific file from the SFTP to Postgres. Prior to this, jarvis will call Amazon titan embedding model and create a vector as we learned in the RAG part.  To do that:</p> <pre><code>curl --location 'https://api.paymentsos.com/hackathon-ai/load' \\\n--header 'private-key: replaceme' \\\n--header 'app-id: replaceme' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"loader\": {\n        \"type\": \"s3\",\n        \"keys\": [\"mastercard.docx\"]\n    },\n    \"collection\": \"hackathon\"\n}'\n</code></pre> <p>After a successful response, those file that we loaded will exist in jarvis, and we will be able to  ask question about them. more on that on the next chapter.</p> <p>collection: By choosing different collection in the API you can isolate different docs, you will be able to specify the collection on your prompts</p>"},{"location":"load/#confluence","title":"Confluence","text":"<p>Jarvis is also directly integrated to our confluence (https://gpo-engineering.atlassian.net/wiki) You can use Jarvis API to stream confluence pages and space in one API call</p> <p>in this example we will publish the whole space but limit to 20 pages</p> <pre><code>curl --location 'https://api.paymentsos.com/hackathon-ai/load' \\\n--header 'private-key: replaceme' \\\n--header 'app-id: replaceme' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"loader\": {\n        \"type\": \"confluence\",\n        \"space\": \"HUB\",\n        \"max_pages\": 20\n    },\n    \"collection\": \"confluence\"\n}'\n</code></pre> <p>and in this example we will choose to load only specific page ids from confluence</p> <pre><code>curl --location 'https://api.paymentsos.com/hackathon-ai/load' \\\n--header 'private-key: replaceme' \\\n--header 'app-id: replaceme' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"loader\": {\n        \"type\": \"confluence\",\n        \"space\": \"HUB\",\n        \"page_ids\": [\"114327634\", \"830275595\"]\n    },\n    \"collection\": \"confluence\"\n}'\n</code></pre> <p>NOTE: Uploading multiple pages from confluence can take some time, and you may get a timeout, we suggest starting with limited number of page_ids. at any case, if there is a timeout from the client, Jarvis will continue uploading the files.</p>"},{"location":"load/#webpages","title":"Webpages","text":"<p>Uploading a webpage is simple as that:  </p> <pre><code>curl --location 'https://api.paymentsos.com/hackathon-ai/load' \\\n--header 'private-key: replaceme' \\\n--header 'app-id: replamce' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"loader\": {\n        \"type\": \"web\",\n        \"urls\": [\"https://medium.com/payu-engineering/unlocking-the-power-of-debezium-69ce9170f101\", \"https://medium.com/payu-engineering/strengthen-the-platforms-security-with-capture-the-flag-aafc4ee12c65\"]\n    },\n    \"collection\": \"website-hackathon\"\n}'\n</code></pre>"},{"location":"privacy/","title":"Privacy","text":""},{"location":"privacy/#data-privacy","title":"Data privacy","text":"<p>if you decide to allow teams to build models based on real personal data stored by PayU you must consider this two points.  </p>"},{"location":"privacy/#preparation-of-training-data","title":"Preparation of training data","text":"<p>Any data science activities should not be done on raw data but on data which was deidentified (or even anonymized) as far as possible (e.g. by removing names, surnames, addresses, hashing of card data, etc.). If a team intends to use any real user data, the scope of the data used and the manner of deidentification should be agreed between you and the relevant team. As we discussed, there is small number of teams that plans to work on PayU personal data to create their models, so the consultation should not be too engaging.</p>"},{"location":"privacy/#safe-environment-for-cooperation","title":"Safe environment for cooperation","text":"<p>Hackathon data science activities should be done in a dedicated safe environment allowing participants to share information and discuss ideas. The tools used by participants should ensure that any information is secure, accessed only by verified users and has appropriate controls limiting export of data. Shortly after the event any personal data should be deleted.</p> <p>Also, please remember that any AI projects should also be assessed from a fairness and ethics perspective, especially with respect to bias control, transparency, explainability, etc. However, given that these models will not be used in production but removed after the hackathon, in this case we can resign from such additional assessments.</p>"},{"location":"simple/","title":"Zero shot prompt","text":""},{"location":"simple/#zero-shot-prompt","title":"Zero shot prompt","text":"<p>Zero-Shot Prompting: If you\u2019ve interacted with an LLM-powered chatbot before, you\u2019ve likely already used zero-shot prompting unwittingly. Zero-shot prompting entails relying solely on an LLM\u2019s pre-trained information to answer a given user prompt.</p> <p>to create your first request to Jarvis you will need a private-key and app-id that will be provided by the hackathon team . This is your authentication keys, and they should be used by your team and remain private.</p> <p>A simple zero shot prompt to jarvis would look like this:</p> <pre><code>curl --location 'https://api.paymentsos.com/hackathon-ai/prompt' \\\n--header 'private-key: replace-me' \\\n--header 'app-id: replace-me' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"question\": \"Can you please tell me what is the most pouplar programing language, please provide short answer\",\n    \"model\": {\n        \"stop_sequences\": [],\n        \"name\": \"anthropic.claude-v2\",\n        \"max_tokens\": 1000,\n        \"temperature\": 1,\n        \"top_k\": 250,\n        \"top_p\": 0.999\n    }\n}'\n</code></pre> <p>and this is the answer you will get:</p> <pre><code>{\n    \"query\": \"Can you please tell me what is the most pouplar programing language, please provide short answer\",\n    \"answer\": \" Python is currently one of the most popular programming languages.\"\n}\n</code></pre> <p>Now lets go over all the params we had in this http request question - This is the prompt that will be sent to the FM stop_sequences - @kobi help me name - This is the model id from the - Bedrock supported models max_tokens - The maximum tokens that will be in the answer - (1 token equal to 3/4 words) temperature - Read here top_k - Read here top_p* - Read here </p> <p>Playing with the request body: different models and different params like temperature, top_k and top_p will give you different answers, play with them and find the right tool for the job.</p> <p>As an example, lets ask FM from AI21 the same question</p> <pre><code>curl --location 'https://api.paymentsos.com/hackathon-ai/prompt' \\\n--header 'private-key: replace-me' \\\n--header 'app-id: replace-me' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"question\": \"Can you please tell me what is the most pouplar programing language, please provide short answer\",\n\"model\": {\n\"stop_sequences\": [],\n\"name\": \"ai21.j2-mid\",\n\"max_tokens\": 1000,\n\"temperature\": 1,\n\"top_k\": 250,\n\"top_p\": 0.999\n}\n}'\n</code></pre> <p>and this is the answer you will get:</p> <pre><code>{\n    \"query\": \"Can you please tell me what is the most pouplar programing language, please provide short answer\",\n    \"answer\": \" According to recent surveys, JavaScript is currently the most popular programming language among developers.\"\n}\n</code></pre> <p>As you can see different models trained and tuned on different data and can return various responses. </p>"}]}